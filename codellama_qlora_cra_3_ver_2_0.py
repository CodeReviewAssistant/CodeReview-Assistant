# -*- coding: utf-8 -*-
"""codellama-qlora-cra-3-ver-2.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WZFsoRK8uuVcQJEOUWtJco2FEMj1J8P_
"""

!pip install transformers datasets bitsandbytes accelerate peft trl -q

import os
import json
import logging
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import torch
from trl import SFTTrainer

def format_dataset(example):
  prompt = f"Please review the following Java code (error type: {example['error_category']}):\n### Code:\n{example['error_code']}\n\n### Review:\n{example['review']}\n\n"
  return {"text": prompt}

dataset = load_dataset("json", data_files="consolidated_synthetic_Project_CodeNet_Java250.jsonl")
dataset = dataset.map(format_dataset, remove_columns=dataset['train'].column_names)

dataset = dataset["train"].train_test_split(test_size=50/1058, shuffle=True)
train_dataset = dataset["train"]
eval_dataset = dataset["test"]

train_dataset, eval_dataset

model_name = "codellama/CodeLlama-7b-Instruct-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side="left")
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=True,
    device_map="auto"
)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

training_args = TrainingArguments(
    output_dir="/content/output",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=5e-4,
    optim="AdamW",
    lr_scheduler_type="cosine",
    logging_steps=10,
    save_steps=200,
    max_grad_norm=1.0,
    save_total_limit=3,
    num_train_epochs=3,
    # report_to="none",
    evaluation_strategy="steps",
    eval_steps=10,
    per_device_eval_batch_size=1,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False
)

trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    args=training_args,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

trainer.train()

trainer.model.save_pretrained("/content/qlora_codellama")
tokenizer.save_pretrained("/content/qlora_codellama")

from huggingface_hub import login
login()

from huggingface_hub import HfApi

repo_id = "dibyansuNh/codellama-qlora-cra3-ver-2.0"

from huggingface_hub import HfApi

api = HfApi()
api.create_repo(repo_id, exist_ok=True)

model.push_to_hub(repo_id)
tokenizer.push_to_hub(repo_id)

print(f"Model uploaded successfully! You can find it at https://huggingface.co/{repo_id}")

"""## Testing model"""

# This code gives a ValueError bcoz the available ram is not enough.
# from transformers import AutoModelForCausalLM, AutoTokenizer
# from peft import PeftModel

# MODEL_REPO = "dibyansuNh/codellama-qlora-cra3-ver-2.0"
# BASE_MODEL = "codellama/CodeLlama-7b-Instruct-hf"

# base_model = AutoModelForCausalLM.from_pretrained(
#     BASE_MODEL,
#     load_in_4bit=True,
#     device_map="auto"
# )

# tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)

# model = PeftModel.from_pretrained(base_model, MODEL_REPO)

# model = model.merge_and_unload()

# print("Model successfully loaded!")

!pip install -U bitsandbytes

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

MODEL_REPO = "dibyansuNh/codellama-qlora-cra3-ver-2.0"
BASE_MODEL = "codellama/CodeLlama-7b-Instruct-hf"

# Enable CPU offloading for the quantized model
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    llm_int8_enable_fp32_cpu_offload=True  # This line is crucial
)

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=quantization_config,  # Pass the quantization config
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)

model = PeftModel.from_pretrained(base_model, MODEL_REPO)

model = model.merge_and_unload()

print("Model successfully loaded!")

# scanner not closed
code_f1 = """
import java.util.Scanner;
public class Main {
    public static void main(String[] args) {
        Scanner scanner = new Scanner(System.in);
        System.out.print("Enter your name: ");
        String name = scanner.nextLine();
        System.out.println("Hello, " + name);
    }
}
"""
# wrong calculation , result * i is correct
code_f2 = """
public class Factorial {
    public static int factorial(int n) {
        int result = 1;
        for (int i = n; i > 0; i--) {
            result = result * n;
        }
        return result;
    }

    public static void main(String[] args) {
        System.out.println(factorial(5));
    }
}
"""
# Inefficient recursion, Slow execution for large numbers
code_f3 = """
public class Fibonacci {
    public static int fibonacci(int n) {
        if (n <= 1) return n;
        return fibonacci(n - 1) + fibonacci(n - 2);
    }

    public static void main(String[] args) {
        System.out.println(fibonacci(30));
    }
}

"""
# SQL Injection vulnerability
code_f4 = """
import java.sql.*;

public class Database {
    public static void main(String[] args) throws Exception {
        Connection conn = DriverManager.getConnection("jdbc:mysql://localhost:3306/test", "user", "password");
        Statement stmt = conn.createStatement();
        String userInput = "admin' --"; // SQL Injection payload
        ResultSet rs = stmt.executeQuery("SELECT * FROM users WHERE username = '" + userInput + "'");
    }
}
"""
# correct fibonacci code
code_c1 = """
import java.util.Scanner;

public class Fibonacci {
    public static void main(String[] args) {
        Scanner scanner = new Scanner(System.in);
        System.out.print("Enter the number of terms: ");

        if (scanner.hasNextInt()) {
            int n = scanner.nextInt();
            int first = 0, second = 1, next;

            System.out.println("Fibonacci Series:");
            for (int i = 0; i < n; i++) {
                System.out.print(first + " ");
                next = first + second;
                first = second;
                second = next;
            }
        } else {
            System.out.println("Invalid input! Please enter an integer.");
        }

        scanner.close();
    }
}

"""

code_c2 = """
class Solution {
    public ListNode insertionSortList(ListNode head) {
        ListNode dummy = new ListNode();
        ListNode curr = head;

        while (curr != null) {
            // At each iteration, we insert an element into the resulting list.
            ListNode prev = dummy;

            // find the position to insert the current node
            while (prev.next != null && prev.next.val <= curr.val) {
                prev = prev.next;
            }

            ListNode next = curr.next;
            // insert the current node to the new list
            curr.next = prev.next;
            prev.next = curr;

            // moving on to the next iteration
            curr = next;
        }

        return dummy.next;
    }
}
"""

def review_code(code_snippet):
    prompt = f"""
    ### Task: Perform a code review for the given java code.
    ### Instructions:
    - Identify any **bugs**, **best practice violations**, or **efficiency improvements**.
    - Suggest **refactored code** if necessary.
    - Provide a **clear and structured review**.
    - Keep the response **concise and informative**.

    ### Code:
    ```java
    {code_snippet}
    Review:
    """

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    output = model.generate(
        inputs["input_ids"],
        max_new_tokens=1024,
        # temperature=0.7,
        # top_p=0.9,
        # do_sample=True,
        repetition_penalty=1.2,
        eos_token_id=tokenizer.eos_token_id
    )

    return tokenizer.decode(output[0], skip_special_tokens=True)

"""# Reviews Generated by CodeLlama-7b-instruct-hf trained on 1008 train samples"""

review = review_code(code_f1)

print("Response:\n")
print(review)

review = review_code(code_c1)

print("Response:\n")
print(review)

review = review_code(code_f2)

print("Response:\n")
print(review)

review = review_code(code_f3)

print("Response:\n")
print(review)

review = review_code(code_f4)

print("Response:\n")
print(review)

review = review_code(code_c2)

print("Response:\n")
print(review)

"""### Reviews generate by CodeLlama-7b-instruct-hf trained on 259 train samples"""

review = review_code(code_f1)

print("Response:\n")
print(review)

review = review_code(code_f2)

print("Response:\n")
print(review)

review = review_code(code_f3)

print("Response:\n")
print(review)

review = review_code(code_f4)

print("Response:\n")
print(review)

review = review_code(code_c1)

print("Response:\n")
print(review)